# -*- coding: utf-8 -*-
"""Rekomendasi_Kesehatan_TERBARU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SFL_UjYfEnyYy9JxB45WJHZhloPXNyLl

# Sistem Rekomendasi Kesehatan

## Dataset: Cardiovascular Disease Detection

**Pipeline:**
1. EDA & Preprocessing
2. Model Training (XGBoost)
3. Model Evaluation
4. Prediction Testing

## Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)
from xgboost import XGBClassifier
import pickle
import warnings
warnings.filterwarnings('ignore')

# Konfigurasi plotting
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

print("Libraries loaded successfully!")

"""---
# PART 1: EDA & PREPROCESSING
---

## 1.1 Load Data
"""

# Load dataset
uploaded = files.upload()

filename = list(uploaded.keys())[0]
df = pd.read_csv(filename, sep=';')

print(f"Dataset shape: {df.shape}")
print(f"\nColumns: {df.columns.tolist()}")
print(f"\nFirst 5 rows:")
df.head()

"""## 1.2 Basic Info"""

# Info dataset
print("Data Types:")
print(df.dtypes)
print("\nMissing Values:")
print(df.isnull().sum())
print("\nStatistical Summary:")
df.describe()

"""## 1.3 Target Distribution"""

# Target distribution
print("Target Distribution:")
print(df['cardio'].value_counts())
print(f"\nPercentage:")
print(df['cardio'].value_counts(normalize=True) * 100)

# Visualisasi
plt.figure(figsize=(8, 6))
df['cardio'].value_counts().plot(kind='bar', color=['green', 'red'])
plt.title('Target Distribution: Cardiovascular Disease', fontsize=14, fontweight='bold')
plt.xlabel('Cardio (0 = No Disease, 1 = Disease)')
plt.ylabel('Count')
plt.xticks([0, 1], ['No Disease', 'Disease'], rotation=0)
plt.tight_layout()
plt.show()

"""## 1.4 Outlier Detection"""

# Check outliers
print("Blood Pressure Outliers:")
print(f"ap_hi - Min: {df['ap_hi'].min()}, Max: {df['ap_hi'].max()}")
print(f"  Data < 60: {len(df[df['ap_hi'] < 60])}, Data > 250: {len(df[df['ap_hi'] > 250])}")
print(f"\nap_lo - Min: {df['ap_lo'].min()}, Max: {df['ap_lo'].max()}")
print(f"  Data < 40: {len(df[df['ap_lo'] < 40])}, Data > 200: {len(df[df['ap_lo'] > 200])}")

# Boxplot
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes[0, 0].boxplot(df['ap_hi'])
axes[0, 0].set_title('Systolic BP', fontweight='bold')
axes[0, 1].boxplot(df['ap_lo'])
axes[0, 1].set_title('Diastolic BP', fontweight='bold')
axes[1, 0].boxplot(df['height'])
axes[1, 0].set_title('Height', fontweight='bold')
axes[1, 1].boxplot(df['weight'])
axes[1, 1].set_title('Weight', fontweight='bold')
plt.tight_layout()
plt.show()

"""## 1.5 Correlation Analysis"""

# Correlation with target
correlation = df.corr()['cardio'].sort_values(ascending=False)
print("Correlation with Target (cardio):")
print(correlation)

# Heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=1, fmt='.2f')
plt.title('Correlation Heatmap', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

"""## 1.6 Data Cleaning"""

original_shape = df.shape
print(f"Original shape: {original_shape}")

# Clean BP
df = df[(df['ap_hi'] >= 60) & (df['ap_hi'] <= 250)]
df = df[(df['ap_lo'] >= 40) & (df['ap_lo'] <= 200)]
df = df[df['ap_hi'] > df['ap_lo']]
print(f"After BP cleaning: {df.shape}")

# Clean Height & Weight
df = df[(df['height'] >= 140) & (df['height'] <= 220)]
df = df[(df['weight'] >= 40) & (df['weight'] <= 200)]
print(f"After height/weight cleaning: {df.shape}")

# Clean Age
df = df[(df['age'] >= 30*365) & (df['age'] <= 70*365)]
print(f"After age cleaning: {df.shape}")

# Remove duplicates
df = df.drop_duplicates()
print(f"After removing duplicates: {df.shape}")

retention = (df.shape[0] / original_shape[0]) * 100
print(f"\nRetention rate: {retention:.2f}%")

"""## 1.7 Feature Engineering"""

# Create new features
df['age_years'] = (df['age'] / 365).round(1)
df['bmi'] = (df['weight'] / ((df['height'] / 100) ** 2)).round(2)
df['pulse_pressure'] = df['ap_hi'] - df['ap_lo']
df['risk_factors'] = (
    (df['smoke'] == 1).astype(int) +
    (df['alco'] == 1).astype(int) +
    (df['cholesterol'] >= 2).astype(int) +
    (df['gluc'] >= 2).astype(int)
)

print("New features created:")
print("  - age_years")
print("  - bmi")
print("  - pulse_pressure")
print("  - risk_factors")

print(f"\nFinal dataset shape: {df.shape}")
df.head()

"""## 1.8 Save & download Cleaned Data"""

df.to_csv('cleaned_data.csv', index=False)
files.download('cleaned_data.csv')
print("Cleaned data saved & downloaded: cleaned_data.csv")

"""---
# PART 2: MODEL TRAINING
---

## 2.1 Define Features & Target
"""

# Features
features = [
    # BASIC EXAMINATION (Mandatory)
    'age_years',        # Usia (tahun)
    'gender',           # Jenis kelamin
    'height',           # Tinggi badan (cm)
    'weight',           # Berat badan (kg)
    'bmi',              # Body Mass Index (calculated)
    'ap_hi',            # Tekanan darah sistolik
    'ap_lo',            # Tekanan darah diastolik
    'pulse_pressure',    # Pulse pressure (calculated)
    # LIFESTYLE (Optional)
    'cholesterol',      # 1: normal, 2: above, 3: well above
    'gluc',             # 1: normal, 2: above, 3: well above
    'smoke',            # 0: no, 1: yes
    'alco',             # 0: no, 1: yes
    'active',           # 0: no, 1: yes
    'risk_factors'      # Calculated from lifestyle
]

target = 'cardio'

X = df[features].copy()
y = df[target].copy()

print(f"Features ({len(features)}): {features}")
print(f"\nX shape: {X.shape}")
print(f"y shape: {y.shape}")
print(f"\nTarget distribution: {y.value_counts().to_dict()}")

"""## 2.2 Train-Test Split"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {X_train.shape}")
print(f"Testing set: {X_test.shape}")
print(f"\nTrain target: {pd.Series(y_train).value_counts().to_dict()}")
print(f"Test target: {pd.Series(y_test).value_counts().to_dict()}")

"""## 2.3 Feature Scaling"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Features scaled with StandardScaler")
print(f"Training set (scaled): {X_train_scaled.shape}")
print(f"Testing set (scaled): {X_test_scaled.shape}")

"""## 2.4 Train XGBoost Model"""

# Model parameters
neg_count = (y_train == 0).sum()
pos_count = (y_train == 1).sum()
scale_pos_weight = neg_count / pos_count * 1.2

params = {
    'n_estimators': 150,              # Increased
    'max_depth': 7,                   # Increased
    'learning_rate': 0.08,            # Decreased
    'subsample': 0.85,
    'colsample_bytree': 0.85,
    'random_state': 42,
    'eval_metric': 'logloss',
    'scale_pos_weight': scale_pos_weight,  # KEY!
    'min_child_weight': 1,
    'gamma': 0.1
}

print("Training XGBoost model...")
model = XGBClassifier(**params)
model.fit(X_train_scaled, y_train, verbose=False)
print("Training completed!")

"""## 2.5 Train Baseline Model (For Comparison)"""

print("="*80)
print("TRAINING BASELINE MODEL (Default XGBoost)")
print("="*80)

# Baseline: XGBoost with default parameters (no tuning)
baseline_params = {
    'n_estimators': 100,
    'max_depth': 6,
    'learning_rate': 0.3,  # Default learning rate
    'random_state': 42,
    'eval_metric': 'logloss'
}

print("\nBaseline Parameters (Default):")
for key, value in baseline_params.items():
    print(f"   {key:20s}: {value}")

print("\nTraining baseline model...")
baseline_model = XGBClassifier(**baseline_params)
baseline_model.fit(X_train_scaled, y_train, verbose=False)
print("Baseline training completed!")

# Save baseline model
import pickle
with open('baseline_model.pkl', 'wb') as f:
    pickle.dump(baseline_model, f)
files.download('baseline_model.pkl')
print("Baseline model saved & downloaded: baseline_model.pkl")

"""## 2.6 Train Additional Models for Comparison"""

print("="*80)
print("TRAINING RANDOM FOREST MODEL")
print("="*80)

from sklearn.ensemble import RandomForestClassifier

# Random Forest with similar complexity to XGBoost
rf_params = {
    'n_estimators': 150,
    'max_depth': 10,
    'min_samples_split': 5,
    'min_samples_leaf': 2,
    'random_state': 42,
    'n_jobs': -1  # Use all CPU cores
}

print("\nRandom Forest Parameters:")
for key, value in rf_params.items():
    print(f"   {key:20s}: {value}")

print("\nTraining Random Forest...")
rf_model = RandomForestClassifier(**rf_params)
rf_model.fit(X_train_scaled, y_train)
print("Random Forest training completed!")

# Save & downloaded model
with open('random_forest_model.pkl', 'wb') as f:
    pickle.dump(rf_model, f)
files.download('random_forest_model.pkl')
print("Random Forest saved & downloaded: random_forest_model.pkl")

# TRAIN SVM
print("\n" + "="*80)
print("TRAINING SVM (Support Vector Machine)")
print("="*80)

from sklearn.svm import SVC

# SVM with RBF kernel (most common for non-linear problems)
svm_params = {
    'C': 1.0,               # Regularization parameter
    'kernel': 'rbf',        # Radial Basis Function
    'gamma': 'scale',       # Auto gamma
    'probability': True,    # Enable probability estimates
    'random_state': 42
}

print("\nSVM Parameters:")
for key, value in svm_params.items():
    print(f"   {key:20s}: {value}")

print("\nTraining SVM (this may take a while on large dataset)...")
svm_model = SVC(**svm_params)
svm_model.fit(X_train_scaled, y_train)
print("SVM training completed!")

# Save & downloaded model
with open('svm_model.pkl', 'wb') as f:
    pickle.dump(svm_model, f)
files.download('svm_model.pkl')
print("SVM saved & downloaded: svm_model.pkl")

print("\n" + "="*80)
print("ALL MODELS TRAINED!")
print("="*80)
print("\nModels available for comparison:")
print("   1. XGBoost (Baseline)")
print("   2. XGBoost (Tuned)")
print("   3. Random Forest")
print("   4. SVM")

"""## 2.7 Feature Importance"""

importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

print("Top 10 Important Features:")
print(importance_df.head(10))

# Plot
plt.figure(figsize=(10, 8))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue', edgecolor='black')
plt.xlabel('Importance', fontweight='bold')
plt.ylabel('Features', fontweight='bold')
plt.title('Feature Importance - XGBoost', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""## 2.8 Download Model & Artifacts"""

for obj, name in [(model, 'xgboost_model.pkl'),
                  (scaler, 'scaler.pkl'),
                  (features, 'feature_names.pkl')]:
    pickle.dump(obj, open(name, 'wb'))
    files.download(name)

print("Model downloaded!")

"""---
# PART 3: MODEL EVALUATION
---

## 3.1 Make Predictions
"""

y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

for threshold in [0.5, 0.45, 0.40, 0.35]:
    y_pred_temp = (y_pred_proba >= threshold).astype(int)
    print(f"Threshold {threshold}: Recall = {recall_score(y_test, y_pred_temp):.4f}")

best_threshold = 0.40
y_pred = (y_pred_proba >= best_threshold).astype(int)

print("Predictions completed")
print(f"Predicted labels: {y_pred.shape}")
print(f"Probabilities: {y_pred_proba.shape}")

"""## 3.2 Multi-Model Comparison (XGBoost vs Random Forest vs SVM)"""

print("="*80)
print("MULTI-MODEL COMPARISON")
print("="*80)

# 1. XGBOOST TUNED
print("\nEvaluating XGBoost (Tuned)...")
y_pred_proba_xgb = model.predict_proba(X_test_scaled)[:, 1]

# Find best threshold for XGBoost
best_threshold_xgb = 0.5
best_f1_xgb = 0
for threshold in [0.5, 0.45, 0.40, 0.35]:
    y_pred_temp = (y_pred_proba_xgb >= threshold).astype(int)
    rec = recall_score(y_test, y_pred_temp)
    f1_temp = f1_score(y_test, y_pred_temp)
    if rec > 0.80 and f1_temp > best_f1_xgb:
        best_f1_xgb = f1_temp
        best_threshold_xgb = threshold

y_pred_xgb = (y_pred_proba_xgb >= best_threshold_xgb).astype(int)

acc_xgb = accuracy_score(y_test, y_pred_xgb)
prec_xgb = precision_score(y_test, y_pred_xgb)
rec_xgb = recall_score(y_test, y_pred_xgb)
f1_xgb = f1_score(y_test, y_pred_xgb)
roc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)

print(f"   Threshold: {best_threshold_xgb}")
print(f"   Recall: {rec_xgb:.4f} ({rec_xgb*100:.2f}%)")

# 2. RANDOM FOREST
print("\nEvaluating Random Forest...")
y_pred_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]
y_pred_rf = rf_model.predict(X_test_scaled)

acc_rf = accuracy_score(y_test, y_pred_rf)
prec_rf = precision_score(y_test, y_pred_rf)
rec_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
roc_rf = roc_auc_score(y_test, y_pred_proba_rf)

print(f"   Recall: {rec_rf:.4f} ({rec_rf*100:.2f}%)")

# 3. SVM
print("\nEvaluating SVM...")
y_pred_proba_svm = svm_model.predict_proba(X_test_scaled)[:, 1]
y_pred_svm = svm_model.predict(X_test_scaled)

acc_svm = accuracy_score(y_test, y_pred_svm)
prec_svm = precision_score(y_test, y_pred_svm)
rec_svm = recall_score(y_test, y_pred_svm)
f1_svm = f1_score(y_test, y_pred_svm)
roc_svm = roc_auc_score(y_test, y_pred_proba_svm)

print(f"   Recall: {rec_svm:.4f} ({rec_svm*100:.2f}%)")

# COMPARISON TABLE
print("\n" + "="*80)
print("COMPARISON RESULTS")
print("="*80)

comparison_data = {
    'Model': ['XGBoost (Tuned)', 'Random Forest', 'SVM'],
    'Accuracy': [
        f"{acc_xgb:.4f} ({acc_xgb*100:.2f}%)",
        f"{acc_rf:.4f} ({acc_rf*100:.2f}%)",
        f"{acc_svm:.4f} ({acc_svm*100:.2f}%)"
    ],
    'Precision': [
        f"{prec_xgb:.4f} ({prec_xgb*100:.2f}%)",
        f"{prec_rf:.4f} ({prec_rf*100:.2f}%)",
        f"{prec_svm:.4f} ({prec_svm*100:.2f}%)"
    ],
    'Recall': [
        f"{rec_xgb:.4f} ({rec_xgb*100:.2f}%)",
        f"{rec_rf:.4f} ({rec_rf*100:.2f}%)",
        f"{rec_svm:.4f} ({rec_svm*100:.2f}%)"
    ],
    'F1-Score': [
        f"{f1_xgb:.4f} ({f1_xgb*100:.2f}%)",
        f"{f1_rf:.4f} ({f1_rf*100:.2f}%)",
        f"{f1_svm:.4f} ({f1_svm*100:.2f}%)"
    ],
    'ROC-AUC': [
        f"{roc_xgb:.4f} ({roc_xgb*100:.2f}%)",
        f"{roc_rf:.4f} ({roc_rf*100:.2f}%)",
        f"{roc_svm:.4f} ({roc_svm*100:.2f}%)"
    ]
}

comparison_df = pd.DataFrame(comparison_data)
print("\n")
print(comparison_df.to_string(index=False))

# BEST MODEL IDENTIFICATION
print("\n" + "="*80)
print("BEST MODEL ANALYSIS")
print("="*80)

models_scores = {
    'XGBoost': {'recall': rec_xgb, 'f1': f1_xgb, 'roc_auc': roc_xgb},
    'Random Forest': {'recall': rec_rf, 'f1': f1_rf, 'roc_auc': roc_rf},
    'SVM': {'recall': rec_svm, 'f1': f1_svm, 'roc_auc': roc_svm}
}

# Find best for each metric
best_recall = max(models_scores, key=lambda x: models_scores[x]['recall'])
best_f1 = max(models_scores, key=lambda x: models_scores[x]['f1'])
best_roc = max(models_scores, key=lambda x: models_scores[x]['roc_auc'])

print(f"\nBest Recall (Most Important for Healthcare): {best_recall}")
print(f"   Score: {models_scores[best_recall]['recall']:.4f} ({models_scores[best_recall]['recall']*100:.2f}%)")

print(f"\nBest F1-Score (Best Balance): {best_f1}")
print(f"   Score: {models_scores[best_f1]['f1']:.4f} ({models_scores[best_f1]['f1']*100:.2f}%)")

print(f"\nBest ROC-AUC (Best Discrimination): {best_roc}")
print(f"   Score: {models_scores[best_roc]['roc_auc']:.4f} ({models_scores[best_roc]['roc_auc']*100:.2f}%)")

# Set the best predictions for further analysis
y_pred = y_pred_xgb
y_pred_proba = y_pred_proba_xgb

"""## 3.3 Multi-Model Visualization"""

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# CHART 1: All Metrics Comparison (Bar Chart)
metrics_list = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
xgb_values = [acc_xgb, prec_xgb, rec_xgb, f1_xgb, roc_xgb]
rf_values = [acc_rf, prec_rf, rec_rf, f1_rf, roc_rf]
svm_values = [acc_svm, prec_svm, rec_svm, f1_svm, roc_svm]

x = np.arange(len(metrics_list))
width = 0.25

axes[0, 0].bar(x - width, xgb_values, width, label='XGBoost', color='#4ECDC4', alpha=0.9)
axes[0, 0].bar(x, rf_values, width, label='Random Forest', color='#95E1D3', alpha=0.9)
axes[0, 0].bar(x + width, svm_values, width, label='SVM', color='#F38181', alpha=0.9)

axes[0, 0].set_xlabel('Metrics', fontweight='bold', fontsize=11)
axes[0, 0].set_ylabel('Score', fontweight='bold', fontsize=11)
axes[0, 0].set_title('Performance Comparison: XGBoost vs Random Forest vs SVM',
                     fontweight='bold', fontsize=13)
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(metrics_list, rotation=45, ha='right')
axes[0, 0].legend()
axes[0, 0].set_ylim([0, 1.0])
axes[0, 0].grid(axis='y', alpha=0.3)

# CHART 2: Recall Comparison
models = ['XGBoost\n(Tuned)', 'Random\nForest', 'SVM']
recall_values = [rec_xgb, rec_rf, rec_svm]
colors_recall = ['#51CF66' if r == max(recall_values) else '#868E96' for r in recall_values]

bars = axes[0, 1].bar(models, recall_values, color=colors_recall, alpha=0.8, edgecolor='black', linewidth=2)
axes[0, 1].set_ylabel('Recall Score', fontweight='bold', fontsize=11)
axes[0, 1].set_title('Recall Comparison (Most Critical for Healthcare)',
                     fontweight='bold', fontsize=13)
axes[0, 1].set_ylim([0, 1.0])
axes[0, 1].axhline(y=0.8, color='red', linestyle='--', label='Target: 80%', linewidth=2)
axes[0, 1].legend()
axes[0, 1].grid(axis='y', alpha=0.3)

for bar, val in zip(bars, recall_values):
    axes[0, 1].text(bar.get_x() + bar.get_width()/2., val + 0.02,
                   f'{val:.1%}', ha='center', va='bottom',
                   fontweight='bold', fontsize=11)

# CHART 3: F1-Score vs ROC-AUC Scatter
f1_scores = [f1_xgb, f1_rf, f1_svm]
roc_scores = [roc_xgb, roc_rf, roc_svm]
colors_scatter = ['#4ECDC4', '#95E1D3', '#F38181']

for model, f1, roc, color in zip(models, f1_scores, roc_scores, colors_scatter):
    axes[1, 0].scatter(f1, roc, s=300, c=color, alpha=0.7, edgecolors='black', linewidth=2)
    axes[1, 0].text(f1 + 0.003, roc, model.replace('\n', ' '),
                   fontsize=10, fontweight='bold')

axes[1, 0].set_xlabel('F1-Score', fontweight='bold', fontsize=11)
axes[1, 0].set_ylabel('ROC-AUC', fontweight='bold', fontsize=11)
axes[1, 0].set_title('F1-Score vs ROC-AUC Trade-off', fontweight='bold', fontsize=13)
axes[1, 0].grid(True, alpha=0.3)
axes[1, 0].set_xlim([min(f1_scores) - 0.02, max(f1_scores) + 0.04])
axes[1, 0].set_ylim([min(roc_scores) - 0.02, max(roc_scores) + 0.02])

axes[1, 1].axis("off")

plt.tight_layout()
plt.show()

# SUMMARY
print("========================================")
print("        MODEL SELECTION SUMMARY         ")
print("========================================")
print(f"WINNER MODEL      : {best_recall}")
print("----------------------------------------")
print(f"Recall            : {models_scores[best_recall]['recall']:.1%}")
print(f"F1-Score          : {models_scores[best_recall]['f1']:.1%}")
print(f"ROC-AUC           : {models_scores[best_recall]['roc_auc']:.1%}")
print("----------------------------------------")
print("========================================")

"""## 3.4 Calculate Metrics"""

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print("="*50)
print("EVALUATION METRICS")
print("="*50)
print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"Precision: {precision:.4f} ({precision*100:.2f}%)")
print(f"Recall:    {recall:.4f} ({recall*100:.2f}%)")
print(f"F1-Score:  {f1:.4f} ({f1*100:.2f}%)")
print(f"ROC-AUC:   {roc_auc:.4f} ({roc_auc*100:.2f}%)")
print("="*50)

"""## 3.5 Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

print("Confusion Matrix:")
print(f"True Negatives:  {tn}")
print(f"False Positives: {fp}")
print(f"False Negatives: {fn}")
print(f"True Positives:  {tp}")

# Plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Disease', 'Disease'],
            yticklabels=['No Disease', 'Disease'])
plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

"""## 3.6 ROC Curve"""

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve', fontsize=14, fontweight='bold')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

"""## 3.7 Classification Report"""

print("Classification Report:")
print(classification_report(y_test, y_pred,
                           target_names=['No Disease', 'Disease'],
                           digits=4))

"""---
# PART 4: PREDICTION TESTING
---

## 4.1 Sample Predictions
"""

# Sample 1: HIGH RISK Profile
print("\n" + "-"*80)
print("SAMPLE 1: HIGH RISK Profile")
print("-"*80)
sample_high = {
    'age': 55,
    'gender': 2,      # Male
    'height': 170,
    'weight': 95,
    'ap_hi': 160,
    'ap_lo': 100
}

print(f"\nInput Data:")
print(f"   Usia: {sample_high['age']} tahun")
print(f"   Gender: {'Pria' if sample_high['gender'] == 2 else 'Wanita'}")
print(f"   Tinggi: {sample_high['height']} cm")
print(f"   Berat: {sample_high['weight']} kg")
print(f"   Tekanan Darah: {sample_high['ap_hi']}/{sample_high['ap_lo']} mmHg")

result_high = predict_health(**sample_high)

print(f"\nHasil Prediksi:")
print(f"   Prediction: {result_high['prediction']}")
print(f"   Risk Level: {result_high['risk_level']}")
print(f"   Confidence: {result_high['confidence']}%")
print(f"   BMI: {result_high['bmi']}")
print(f"   Pulse Pressure: {result_high['pulse_pressure']} mmHg")
print(f"   Prob(No Disease): {result_high['prob_no_disease']}%")
print(f"   Prob(Disease): {result_high['prob_disease']}%")

# Sample 2: LOW RISK Profile
print("\n" + "-"*80)
print("SAMPLE 2: LOW RISK Profile")
print("-"*80)
sample_low = {
    'age': 35,
    'gender': 1,      # Female
    'height': 165,
    'weight': 60,
    'ap_hi': 115,
    'ap_lo': 75
}

print(f"\nInput Data:")
print(f"   Usia: {sample_low['age']} tahun")
print(f"   Gender: {'Pria' if sample_low['gender'] == 2 else 'Wanita'}")
print(f"   Tinggi: {sample_low['height']} cm")
print(f"   Berat: {sample_low['weight']} kg")
print(f"   Tekanan Darah: {sample_low['ap_hi']}/{sample_low['ap_lo']} mmHg")

result_low = predict_health(**sample_low)

print(f"\nHasil Prediksi:")
print(f"   Prediction: {result_low['prediction']}")
print(f"   Risk Level: {result_low['risk_level']}")
print(f"   Confidence: {result_low['confidence']}%")
print(f"   BMI: {result_low['bmi']}")
print(f"   Pulse Pressure: {result_low['pulse_pressure']} mmHg")
print(f"   Prob(No Disease): {result_low['prob_no_disease']}%")
print(f"   Prob(Disease): {result_low['prob_disease']}%")

# Sample 3: MEDIUM RISK Profile
print("\n" + "-"*80)
print("SAMPLE 3: MEDIUM RISK Profile")
print("-"*80)
sample_med = {
    'age': 45,
    'gender': 2,      # Male
    'height': 175,
    'weight': 80,
    'ap_hi': 135,
    'ap_lo': 85
}

print(f"\nInput Data:")
print(f"   Usia: {sample_med['age']} tahun")
print(f"   Gender: {'Pria' if sample_med['gender'] == 2 else 'Wanita'}")
print(f"   Tinggi: {sample_med['height']} cm")
print(f"   Berat: {sample_med['weight']} kg")
print(f"   Tekanan Darah: {sample_med['ap_hi']}/{sample_med['ap_lo']} mmHg")

result_med = predict_health(**sample_med)

print(f"\nHasil Prediksi:")
print(f"   Prediction: {result_med['prediction']}")
print(f"   Risk Level: {result_med['risk_level']}")
print(f"   Confidence: {result_med['confidence']}%")
print(f"   BMI: {result_med['bmi']}")
print(f"   Pulse Pressure: {result_med['pulse_pressure']} mmHg")
print(f"   Prob(No Disease): {result_med['prob_no_disease']}%")
print(f"   Prob(Disease): {result_med['prob_disease']}%")

"""## 4.2 Custom Prediction Function"""

def predict_health(age, gender, height, weight, ap_hi, ap_lo, cholesterol=None, gluc=None, smoke=None, alco=None, active=None):
    """
    Predict cardiovascular disease risk with OPTIONAL lifestyle features

    MANDATORY Parameters (6):
    - age: years
    - gender: 1=Female, 2=Male
    - height: (cm)
    - weight: (kg)
    - ap_hi: systolic blood pressure
    - ap_lo: diastolic blood pressure

    OPTIONAL Parameters (5):
    - cholesterol: 1=normal, 2=above, 3=well above (default: None = assumed normal)
    - gluc: 1=normal, 2=above, 3=well above (default: None = assumed normal)
    - smoke: 0=no, 1=yes (default: None = assumed no)
    - alco: 0=no, 1=yes (default: None = assumed no)
    - active: 0=no, 1=yes (default: None = assumed yes/active)
    """
    # Calculate derived features
    bmi = round(weight / ((height/100) ** 2), 2)
    pulse_pressure = ap_hi - ap_lo

    # Handle optional lifestyle features (set to healthy defaults if not provided)
    cholesterol = cholesterol if cholesterol is not None else 1  # Normal
    gluc = gluc if gluc is not None else 1  # Normal
    smoke = smoke if smoke is not None else 0  # Non-smoker
    alco = alco if alco is not None else 0  # No alcohol
    active = active if active is not None else 1  # Active (default healthy)

    # Calculate risk_factors from lifestyle
    risk_factors = (
        (smoke == 1) +
        (alco == 1) +
        (cholesterol >= 2) +
        (gluc >= 2)
    )

    # Prepare data (14 features)
    data = np.array([[
        age, gender, height, weight, bmi,
        ap_hi, ap_lo, pulse_pressure,
        cholesterol, gluc, smoke, alco, active,
        risk_factors
    ]])

    # Scale & predict
    data_scaled = scaler.transform(data)
    prediction = model.predict(data_scaled)[0]
    probabilities = model.predict_proba(data_scaled)[0]

    result = {
        'prediction': 'DISEASE' if prediction == 1 else 'NO DISEASE',
        'risk_level': 'HIGH' if prediction == 1 else 'LOW',
        'confidence': round(max(probabilities) * 100, 2),
        'prob_no_disease': round(probabilities[0] * 100, 2),
        'prob_disease': round(probabilities[1] * 100, 2),
        'bmi': bmi,
        'pulse_pressure': pulse_pressure,
        'risk_factors': risk_factors,
        'lifestyle_data': {
            'cholesterol': 'normal' if cholesterol == 1 else ('above normal' if cholesterol == 2 else 'well above normal'),
            'glucose': 'normal' if gluc == 1 else ('above normal' if gluc == 2 else 'well above normal'),
            'smoking': 'yes' if smoke == 1 else 'no',
            'alcohol': 'yes' if alco == 1 else 'no',
            'physical_activity': 'yes' if active == 1 else 'no'
        }
    }

    return result

# Test function
result = predict_health(
    age=50, gender=2, height=175, weight=80,
    ap_hi=140, ap_lo=90
)

print("Test Prediction:")
print(result)